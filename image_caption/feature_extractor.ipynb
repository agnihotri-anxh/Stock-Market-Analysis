{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c164ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d462e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "635f1868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Setting up paths and loading data...\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Setup Paths and Load Data ---\n",
    "print(\"Step 1: Setting up paths and loading data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2fb5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relative paths and handle different execution directories\n",
    "annotations_file = 'dataset/annotations/captions_train2017.json'\n",
    "image_dir = 'dataset/train2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a9ca1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found in current directory. Assuming execution from 'image_caption' subdirectory.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(annotations_file):\n",
    "    print(\"Dataset not found in current directory. Assuming execution from 'image_caption' subdirectory.\")\n",
    "    annotations_file = os.path.join('..', annotations_file)\n",
    "    image_dir = os.path.join('..', image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8f01b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check for the annotations file\n",
    "if not os.path.exists(annotations_file):\n",
    "    raise FileNotFoundError(f\"Error: The annotations file was not found at the expected path: {os.path.abspath(annotations_file)}. Please make sure you are running this from the project root or the 'image_caption' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "922f96cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations from d:\\Zidio_development\\dataset\\annotations\\captions_train2017.json...\n"
     ]
    }
   ],
   "source": [
    "# Load annotations directly\n",
    "print(f\"Loading annotations from {os.path.abspath(annotations_file)}...\")\n",
    "with open(annotations_file, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4120068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Captions: 100%|██████████| 591753/591753 [00:01<00:00, 485561.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from image ID to captions\n",
    "mapping = {}\n",
    "for annot in tqdm(data['annotations'], desc=\"Loading Captions\"):\n",
    "    image_id = str(annot['image_id'])\n",
    "    caption = annot['caption']\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb0748c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Preprocessing text data...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Preprocess Text Data ---\n",
    "print(\"\\nStep 2: Preprocessing text data...\")\n",
    "def preprocess_captions(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            caption = captions[i].lower()\n",
    "            caption = caption.replace('[^A-Za-z]', '')\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word) > 1]) + ' endseq'\n",
    "            captions[i] = caption\n",
    "preprocess_captions(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a551e8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed captions mapping\n"
     ]
    }
   ],
   "source": [
    "# Save the processed mapping for later use\n",
    "with open('processed_captions_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(mapping, f)\n",
    "print(\"Saved processed captions mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aee0bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Extracting image features with VGG16...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Extract Image Features with VGG16 ---\n",
    "print(\"\\nStep 3: Extracting image features with VGG16...\")\n",
    "vgg_model = VGG16()\n",
    "vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01c744fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "image_files = os.listdir(image_dir)\n",
    "image_files = image_files[:20000]  # Process first 20000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "922131fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20000 images...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing {len(image_files)} images...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f179007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 20000/20000 [54:59<00:00,  6.06it/s] \n"
     ]
    }
   ],
   "source": [
    "for img_name in tqdm(image_files, desc=\"Extracting features\"):\n",
    "    img_path = os.path.join(image_dir, img_name)\n",
    "    try:\n",
    "        image = load_img(img_path, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        image = preprocess_input(image)\n",
    "        feature = vgg_model.predict(image, verbose=0)\n",
    "        \n",
    "        # Extract image ID from filename (assuming COCO format, e.g., '000000397133.jpg')\n",
    "        image_id = img_name.split('.')[0].lstrip('0')\n",
    "        features[image_id] = feature\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_name}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac885966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving features for 20000 images...\n",
      "Successfully saved VGG16 features to 'vgg16_features_20000.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Save the features after processing all images\n",
    "print(f\"\\nSaving features for {len(features)} images...\")\n",
    "with open('vgg16_features_20000.pkl', 'wb') as f:\n",
    "    pickle.dump(features, f)\n",
    "print(f\"Successfully saved VGG16 features to 'vgg16_features_20000.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f1f4fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved original annotations data\n"
     ]
    }
   ],
   "source": [
    "# Also save the original data for later use\n",
    "with open('original_annotations_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "print(\"Saved original annotations data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3987aa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction completed successfully!\n",
      "Files created:\n",
      "- vgg16_features_20000.pkl: VGG16 features for images\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFeature extraction completed successfully!\")\n",
    "print(\"Files created:\")\n",
    "print(\"- vgg16_features_20000.pkl: VGG16 features for images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578562e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "img_caption1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
